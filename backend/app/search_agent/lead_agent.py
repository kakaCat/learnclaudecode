from __future__ import annotations
import json
import logging
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

from langchain_core.messages import HumanMessage, SystemMessage

from backend.app.llm import get_llm
from backend.app.search_agent.search_subagent import SearchSubagent
from backend.app.search_agent.citation_agent import CitationAgent
from backend.app.search.duckduckgo import DuckDuckGoSearch

logger = logging.getLogger(__name__)

G = "\033[90m"
R = "\033[0m"

DEFAULT_RESEARCH_DIR = Path("scripts/research")

# æŸ¥è¯¢ç±»å‹ â†’ subagent æ•°é‡ä¸Šé™
_SUBAGENT_COUNT = {"direct": 1, "broad": 3, "deep": 5}
# æŸ¥è¯¢ç±»å‹ â†’ subagent difficulty
_DIFFICULTY = {"direct": "simple", "broad": "medium", "deep": "hard"}


class SearchLeadAgent:
    """
    ç ”ç©¶ç¼–æ’ agentï¼Œéµå¾ªã€Œè¯„ä¼°-åˆ†ç±»-è®¡åˆ’-æ‰§è¡Œã€å››æ­¥å¾ªç¯ã€‚

    pre-loop:
      è¯„ä¼°  â†’ probe()     æ¢ç´¢ä¿¡æ¯ç‰ˆå›¾ï¼Œç†è§£æ•°æ®ç»“æ„
      åˆ†ç±»  â†’ classify()  åˆ¤æ–­æŸ¥è¯¢ç±»å‹ï¼ˆdirect / broad / deepï¼‰

    loop (up to MAX_CYCLES):
      è®¡åˆ’  â†’ _plan_queries()        æŒ‰ç±»å‹ç”ŸæˆæŸ¥è¯¢ï¼Œé’ˆå¯¹ gaps è¡¥æœ
      æ‰§è¡Œ  â†’ _dispatch()            å¹¶è¡Œ spawn SearchSubagent
      ç›‘æ§  â†’ _evaluate_coverage()   è¯„ä¼°è¦†ç›–åº¦ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£
      é€‚åº”  â†’ _adapt()               è´å¶æ–¯æ›´æ–°ï¼šç»§ç»­ or æ­¢æŸç»¼åˆ

    æœç´¢ç»“æœå­˜å…¥æ–‡ä»¶ç³»ç»Ÿï¼Œä»…å‘è°ƒç”¨æ–¹è¿”å›è½»é‡çº§æ–‡ä»¶è·¯å¾„ã€‚
    Lead ä¸æ‰§è¡Œåˆçº§æœç´¢ï¼Œåªè´Ÿè´£è§„åˆ’ã€å§”æ´¾ã€æ•´åˆã€‚
    """

    MAX_CYCLES = 4

    def __init__(self):
        self._llm = get_llm()
        self._ddg = DuckDuckGoSearch(max_results=3)   # ä»…ç”¨äºæ¢ç´¢æ¨¡å¼ probe
        self._citation = CitationAgent()

    def run(self, topic: str, research_dir: Path | None = None) -> str:
        print(f"{G}ğŸ” [SearchLeadAgent] topic: {topic}{R}")
        _research_dir = research_dir or DEFAULT_RESEARCH_DIR
        self._research_dir = _research_dir   # ä¾› _dispatch ä¼ ç»™ SubAgent

        # â”€â”€ æ¢ç´¢æ¨¡å¼ï¼šå§”æ´¾å‰å…ˆ probeï¼Œç†è§£ä¿¡æ¯ç‰ˆå›¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        probe_hint = self._probe(topic)
        print(f"{G}   ğŸ”­ probe hint: {probe_hint[:120]}{R}")

        # â”€â”€ æŸ¥è¯¢ç±»å‹åˆ†ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        query_type = self._classify(topic, probe_hint)
        max_subagents = _SUBAGENT_COUNT.get(query_type, 3)
        difficulty = _DIFFICULTY.get(query_type, "medium")
        print(f"{G}   ğŸ·  type={query_type}, max_subagents={max_subagents}, difficulty={difficulty}{R}")

        memory: dict = {
            "topic": topic,
            "query_type": query_type,
            "cycles": [],
            "searched": [],
        }
        all_results: dict[str, str] = {}

        for cycle in range(1, self.MAX_CYCLES + 1):
            print(f"{G}   ğŸ”„ [Research] cycle {cycle}/{self.MAX_CYCLES}{R}")

            # â”€â”€ è®¡åˆ’ï¼šç”Ÿæˆæœ¬è½®æŸ¥è¯¢ï¼Œé’ˆå¯¹ gaps è¡¥æœ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            gaps = memory["cycles"][-1]["gaps"] if memory["cycles"] else []
            queries = self._plan_queries(topic, memory["searched"], all_results, gaps, query_type)
            new_queries = [q for q in queries if q not in memory["searched"]]
            new_queries = new_queries[:max_subagents]

            # â”€â”€ æ‰§è¡Œï¼šå¹¶è¡Œ dispatch subagent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if new_queries:
                print(f"{G}   ğŸ“¤ dispatch ({len(new_queries)}): {new_queries}{R}")
                batch = self._dispatch(new_queries, topic=topic, difficulty=difficulty)
                all_results.update(batch)
                memory["searched"].extend(new_queries)

            # â”€â”€ ç›‘æ§ï¼šè¯„ä¼°è¦†ç›–åº¦ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            situation = self._evaluate_coverage(topic, all_results)
            confidence = situation.get("confidence", 0.5)
            gaps = situation.get("gaps", [])
            print(f"{G}   ğŸ“Š confidence={confidence:.2f}, gaps={gaps}{R}")
            memory["cycles"].append({"cycle": cycle, "confidence": confidence, "gaps": gaps})

            # â”€â”€ é€‚åº”ï¼šè´å¶æ–¯æ›´æ–°ï¼Œå†³å®šç»§ç»­è¿˜æ˜¯æ­¢æŸç»¼åˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            choice = self._adapt(situation, cycle, memory["searched"])
            print(f"{G}   ğŸ¯ adapt={choice}{R}")

            if choice == "SYNTHESIZE":
                break

        path = self._synthesize(topic, all_results, memory, _research_dir)
        self._citation.run(path)
        print(f"{G}   âœ… saved â†’ {path}{R}")
        return path

    # â”€â”€ æ¢ç´¢æ¨¡å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _probe(self, topic: str) -> str:
        """å§”æ´¾å‰å…ˆç”¨1æ¬¡æœç´¢æ¢ç´¢ä¿¡æ¯ç‰ˆå›¾ï¼Œåªè¿”å›æ ‡é¢˜+URLåˆ—è¡¨ï¼Œä¾› classify åˆ¤æ–­æŸ¥è¯¢ç±»å‹ã€‚"""
        raw = self._ddg.search(topic)
        lines = [f"{i}. {r['title']} â€” {r['url']}" for i, r in enumerate(raw, 1)]
        return "\n".join(lines)

    # â”€â”€ æŸ¥è¯¢ç±»å‹åˆ†ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _classify(self, topic: str, probe_hint: str) -> str:
        """
        åˆ¤æ–­æŸ¥è¯¢ç±»å‹ï¼š
        - direct: èšç„¦ã€å®šä¹‰æ˜ç¡®ï¼Œå•æ¬¡è°ƒæŸ¥å³å¯
        - broad:  ç‹¬ç«‹å­é—®é¢˜ï¼Œéœ€è¦åˆ†å¤´æŠ“å–æ•°æ®å†æ±‡æ€»
        - deep:   å¤šè§†è§’æ·±æŒ–å•ä¸€è¯é¢˜ï¼Œéœ€è¦å¹³è¡Œæ¢ç´¢ä¸åŒè§‚ç‚¹
        """
        resp = self._llm.invoke([
            SystemMessage(content=(
                "Classify the research topic into one of three types.\n"
                'Output ONLY valid JSON: {"type": "direct" | "broad" | "deep", "reason": "one sentence"}\n'
                "- direct: focused, well-defined, single investigation suffices\n"
                "- broad: multiple independent sub-questions, need parallel data gathering\n"
                "- deep: single topic requiring multi-perspective analysis"
            )),
            HumanMessage(content=f"Topic: {topic}\n\nProbe results:\n{probe_hint[:1000]}"),
        ])
        try:
            raw = resp.content.strip().strip("```json").strip("```").strip()
            return json.loads(raw).get("type", "broad")
        except (json.JSONDecodeError, AttributeError):
            return "broad"

    # â”€â”€ ç ”ç©¶å¾ªç¯å››é˜¶æ®µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _plan_queries(
        self,
        topic: str,
        searched: list[str],
        results: dict[str, str],
        gaps: list[str],
        query_type: str,
    ) -> list[str]:
        # æŒ‰æŸ¥è¯¢ç±»å‹ç»™ LLM ä¸åŒçš„è§„åˆ’ç­–ç•¥
        strategy = {
            "direct": "Generate 1 focused query that directly answers the topic.",
            "broad":  "Decompose into 2-4 independent sub-questions covering different aspects.",
            "deep":   "Generate 3-5 queries exploring different perspectives, methodologies, or expert views.",
        }.get(query_type, "Generate 2-4 queries covering the topic.")

        gap_hint = f"\nKnown gaps to fill: {gaps}" if gaps else ""
        context = ""
        if results:
            # å‹ç¼©ï¼šåªä¼ æ‘˜è¦è€Œéå…¨æ–‡ï¼Œé¿å…ä¸Šä¸‹æ–‡è†¨èƒ€
            context = "\nCurrent coverage summary:\n" + _summarize(results)

        resp = self._llm.invoke([
            SystemMessage(content=(
                "You are a research planner.\n"
                f"Strategy: {strategy}\n"
                "Never repeat already-searched queries.\n"
                'Output ONLY valid JSON: {"queries": ["q1", "q2", ...]}\n'
                'If nothing new to search: {"queries": []}'
            )),
            HumanMessage(content=(
                f"Topic: {topic}\n"
                f"Query type: {query_type}\n"
                f"Already searched: {searched}"
                f"{gap_hint}"
                f"{context}"
            )),
        ])
        try:
            raw = resp.content.strip().strip("```json").strip("```").strip()
            return json.loads(raw).get("queries", [])
        except (json.JSONDecodeError, AttributeError):
            return [topic] if not searched else []

    def _evaluate_coverage(self, topic: str, results: dict[str, str]) -> dict:
        resp = self._llm.invoke([
            SystemMessage(content=(
                "You are a research evaluator.\n"
                "Output ONLY valid JSON:\n"
                '{"situation": "brief summary", "gaps": ["gap1", ...], "confidence": 0.0-1.0}\n'
                "confidence: 0.0=very incomplete, 1.0=fully covered"
            )),
            HumanMessage(content=(
                f"Topic: {topic}\n\n"
                f"Results:\n{_aggregate(results)[:4000]}"
            )),
        ])
        try:
            raw = resp.content.strip().strip("```json").strip("```").strip()
            return json.loads(raw)
        except (json.JSONDecodeError, AttributeError):
            return {"situation": "", "gaps": [], "confidence": 0.7}

    def _adapt(self, situation: dict, cycle: int, searched: list[str]) -> str:
        confidence = situation.get("confidence", 0.7)
        gaps = situation.get("gaps", [])

        # å¼ºåˆ¶ç»ˆæ­¢ï¼šæœ€åä¸€è½® or æ— æ–° gapï¼ˆè¾¹é™…æ”¶ç›Šé€’å‡ï¼‰
        if cycle >= self.MAX_CYCLES or not gaps:
            return "SYNTHESIZE"
        # æœç´¢é‡è¿‡å¤šæ—¶åŠæ—¶æ­¢æŸ
        if len(searched) >= 10:
            return "SYNTHESIZE"
        # è´å¶æ–¯ï¼šé«˜ç½®ä¿¡åº¦ç›´æ¥ç»¼åˆ
        if confidence >= 0.75:
            return "SYNTHESIZE"
        return "OBSERVE_MORE"

    def _synthesize(self, topic: str, results: dict[str, str], memory: dict, research_dir: Path) -> str:
        aggregated = _aggregate(results)
        resp = self._llm.invoke([
            SystemMessage(content=(
                "You are a research synthesizer.\n"
                "Write a concise markdown research report.\n"
                "Include: key findings, common themes, notable sources (URLs).\n"
                "Be factual and cite URLs where relevant."
            )),
            HumanMessage(content=f"Topic: {topic}\n\nSearch results:\n{aggregated}"),
        ])
        summary = resp.content.strip()

        slug = re.sub(r"[^\w\u4e00-\u9fff]+", "_", topic)[:40]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = research_dir / f"{slug}_{timestamp}.md"
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(
            f"# Research: {topic}\n\n"
            f"{summary}\n\n"
            f"---\n\n"
            f"## Research Memory\n\n```json\n{json.dumps(memory, ensure_ascii=False, indent=2)}\n```\n\n"
            f"---\n\n"
            f"## Raw Results\n\n{aggregated}"
        )
        return str(file_path)

    # â”€â”€ å¹¶è¡Œå§”æ´¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _dispatch(
        self, queries: list[str], topic: str = "", difficulty: str = "medium"
    ) -> dict[str, str]:
        """æ¯ä¸ª query åˆ›å»ºç‹¬ç«‹ SearchSubagent å®ä¾‹ï¼ˆç‹¬ç«‹ä¸Šä¸‹æ–‡ï¼‰ï¼Œå¹¶è¡Œæ‰§è¡Œã€‚"""
        results: dict[str, str] = {}

        def _run(q: str) -> tuple[str, str]:
            # æ¯æ¬¡è°ƒç”¨åˆ›å»ºæ–°å®ä¾‹ï¼Œä¿è¯ç‹¬ç«‹ä¸Šä¸‹æ–‡ï¼›ç»“æœå†™å…¥æ–‡ä»¶ï¼Œè¿”å›è·¯å¾„
            agent = SearchSubagent(difficulty=difficulty)
            return q, agent.run(q, topic=topic, research_dir=self._research_dir)

        with ThreadPoolExecutor(max_workers=len(queries)) as executor:
            futures = {executor.submit(_run, q): q for q in queries}
            for future in as_completed(futures):
                q, result = future.result()
                results[q] = result
                logger.info("subagent done: %s", q[:60])

        return results


# â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _aggregate(results: dict[str, str]) -> str:
    lines = []
    for i, (query, path_or_text) in enumerate(results.items(), 1):
        lines.append(f"## Query {i}: {query}")
        content = _read_result(path_or_text)
        lines.append(content)
        lines.append("")
    return "\n".join(lines).strip()


def _summarize(results: dict[str, str], max_chars: int = 3000) -> str:
    """å‹ç¼©æ‘˜è¦ï¼šæ¯æ¡æŸ¥è¯¢åªä¿ç•™å‰ N å­—ç¬¦ï¼Œé¿å… evaluate_coverage é˜¶æ®µä¸Šä¸‹æ–‡è†¨èƒ€ã€‚"""
    lines = []
    per_query = max(300, max_chars // max(len(results), 1))
    for query, path_or_text in results.items():
        content = _read_result(path_or_text)
        lines.append(f"[{query}]: {content[:per_query]}")
    return "\n".join(lines)


def _read_result(path_or_text: str) -> str:
    """è‹¥å€¼æ˜¯å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„åˆ™è¯»å–æ–‡ä»¶ï¼Œå¦åˆ™ç›´æ¥è¿”å›æ–‡æœ¬ã€‚"""
    p = Path(path_or_text)
    if p.exists():
        return p.read_text()
    return path_or_text
