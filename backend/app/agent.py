import json
import logging

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain.agents import create_agent
from backend.app.config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, DEEPSEEK_MODEL
from backend.app.prompts import SYSTEM_PROMPT
from backend.app.tools import TOOLS
from backend.app.compact import was_compact_requested
from backend.app.background import drain_notifications
from backend.app.team import get_bus
from backend.app.team import state as _team_state
from backend.app.compaction import estimate_tokens, micro_compact, auto_compact
from backend.app.session import new_session_key, set_session_key, save_session

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")

G = "\033[90m"
R = "\033[0m"

THRESHOLD = 50000

TOOL_ICONS = {
    "bash": "ğŸ’»", "read_file": "ğŸ“–", "write_file": "âœï¸", "edit_file": "âœï¸",
    "glob": "ğŸ”", "grep": "ğŸ”", "list_dir": "ğŸ“‚",
    "Task": "ğŸ¤–", "load_skill": "ğŸ“š", "compact": "ğŸ—œï¸",
    "task_create": "ğŸ“Œ", "task_get": "ğŸ”–", "task_update": "ğŸ”„", "task_list": "ğŸ“",
    "task_bind_worktree": "ğŸ”—",
    "background_run": "âš¡", "check_background": "ğŸ“¡",
    "worktree_create": "ğŸŒ¿", "worktree_list": "ğŸŒ³", "worktree_status": "ğŸ“Š",
    "worktree_run": "â–¶ï¸", "worktree_remove": "ğŸ—‘ï¸", "worktree_keep": "ğŸ“",
    "worktree_events": "ğŸ“œ",
}


def _log(icon: str, msg: str):
    print(f"{G}{icon} {msg}{R}")


def _build_agent():
    llm = ChatOpenAI(
        model=DEEPSEEK_MODEL,
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL,
    )
    return create_agent(llm, TOOLS, system_prompt=SYSTEM_PROMPT), llm


class AgentService:
    def __init__(self):
        self.agent, self.llm = _build_agent()
        self.session_key = new_session_key()
        set_session_key(self.session_key)
        self.rounds_without_todo = 0
        _log("ğŸ¤–", f"Agent å°±ç»ª | æ¨¡å‹={DEEPSEEK_MODEL} | session={self.session_key}")

    def run(self, prompt: str, history: list = None) -> str:
        if history is None:
            history = []

        # Layer 1: micro_compact
        micro_compact(history)
        # Layer 2: auto_compact
        if estimate_tokens(history, self.llm) > THRESHOLD:
            _log("ğŸ—œï¸", "[auto_compact triggered]")
            new_history = auto_compact(history, self.llm)
            history.clear()
            history.extend(new_history)

        # æ³¨å…¥ lead inbox æ¶ˆæ¯ï¼ˆä»…å½“ team å·²åˆå§‹åŒ–æ—¶ï¼Œé¿å…æå‰åˆ›å»º team ç›®å½•ï¼‰
        inbox = get_bus().read_inbox("lead") if _team_state._bus is not None else []
        if inbox and history:
            history.append(HumanMessage(content=f"<inbox>{json.dumps(inbox, indent=2)}</inbox>"))
            history.append(AIMessage(content="Noted inbox messages."))
            _log("ğŸ“¬", f"æ³¨å…¥ {len(inbox)} æ¡ inbox æ¶ˆæ¯")

        # æ³¨å…¥åå°ä»»åŠ¡å®Œæˆé€šçŸ¥
        notifs = drain_notifications()
        if notifs and history:
            notif_text = "\n".join(
                f"[bg:{n['task_id']}] {n['status']}: {n['result']}" for n in notifs
            )
            history.append(HumanMessage(content=f"<background-results>\n{notif_text}\n</background-results>"))
            history.append(AIMessage(content="Noted background results."))
            _log("ğŸ“¡", f"æ³¨å…¥ {len(notifs)} æ¡åå°ä»»åŠ¡é€šçŸ¥")

        _log("ğŸ‘¤", f"ç”¨æˆ·è¾“å…¥: {prompt}")

        messages = history + [HumanMessage(content=prompt)]
        if self.rounds_without_todo >= 3:
            messages.append(HumanMessage(content="<reminder>è¯·æ›´æ–°ä½ çš„ TodoWrite å¾…åŠäº‹é¡¹ã€‚</reminder>"))
        output = ""
        turn = 0
        last_state_messages = messages
        tool_results_summary = []

        for step in self.agent.stream({"messages": messages}, stream_mode="updates"):
            for node, state in step.items():
                last = state["messages"][-1]
                if node == "agent":
                    turn += 1
                    last_state_messages = state["messages"]
                    _log("ğŸ§ ", f"[ç¬¬ {turn} æ¬¡è°ƒç”¨ LLM] ä¸Šä¸‹æ–‡æ¶ˆæ¯æ•°={len(state['messages'])}")
                    if getattr(last, "tool_calls", None):
                        for tc in last.tool_calls:
                            icon = TOOL_ICONS.get(tc["name"], "ğŸ”§")
                            _log("ğŸ”€", f"  AI å†³ç­–: è°ƒç”¨å·¥å…· {icon}[{tc['name']}] å‚æ•°={tc['args']}")
                    else:
                        output = last.content or output
                        _log("ğŸ§ ", f"  AI å†³ç­–: ç›´æ¥å›ç­”ï¼Œæ— éœ€å·¥å…·")
                elif node == "tools":
                    _log("ğŸ“¥", f"  å·¥å…·è¿”å›: {last.content[:80]}")
                    tool_results_summary.append(last.content[:500])
                    if last.name == "TodoWrite":
                        self.rounds_without_todo = 0
                    else:
                        self.rounds_without_todo += 1
                    # drain after each tool batch (mirrors v7: drain before each LLM call)
                    notifs = drain_notifications()
                    if notifs:
                        notif_text = "\n".join(
                            f"[bg:{n['task_id']}] {n['status']}: {n['result']}" for n in notifs
                        )
                        messages = last_state_messages + [
                            HumanMessage(content=f"<background-results>\n{notif_text}\n</background-results>"),
                            AIMessage(content="Noted background results."),
                        ]
                        _log("ğŸ“¡", f"  åŒè½®æ³¨å…¥ {len(notifs)} æ¡åå°ä»»åŠ¡é€šçŸ¥")

        # DeepSeek sometimes returns empty content after tool use â€” call LLM once more
        if not output:
            _log("ğŸ§ ", "  [è¡¥å……è°ƒç”¨] è·å–æœ€ç»ˆå›ç­”")
            tool_context = "\n".join(f"- {r}" for r in tool_results_summary)
            fallback_messages = last_state_messages + [
                HumanMessage(content=f"å·¥å…·è°ƒç”¨ç»“æœå¦‚ä¸‹ï¼š\n{tool_context}\n\nè¯·æ ¹æ®ä»¥ä¸Šç»“æœï¼Œç”¨ä¸­æ–‡ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œç›´æ¥å¼•ç”¨å·¥å…·è¿”å›çš„åŸå§‹æ•°æ®ï¼Œä¸è¦ç¼–é€ ä»»ä½•IDæˆ–æ•°å€¼ã€‚")
            ]
            resp = self.llm.invoke(fallback_messages)
            output = resp.content

        history.append(HumanMessage(content=prompt))
        history.append(AIMessage(content=output))

        # Layer 3: manual compact triggered by compact tool
        if was_compact_requested():
            _log("ğŸ—œï¸", "[manual compact]")
            new_history = auto_compact(history, self.llm)
            history.clear()
            history.extend(new_history)

        _log("âœ…", f"AI æœ€ç»ˆå›ç­” â†’ {output[:120]}")
        save_session("main", history)
        return output
